\chapter{Design and Implementation}
\label{ch:design-and-impl}
Referring to the roadmap of this project, during the first semester I finished stage 1 (framework) and stage 2 (platform). Due to the page length restriction, I am providing an overview and one notable design detail of these subprojects in this report. Additional details on API documentation and design considerations can be found in these external documents:
\begin{enumerate}
    \item \href{https://yuanhong.larksuite.com/docs/docusSYdnLXZBojin39b8DGzKMT}{Gym} (Password: mLpj)
    \item \href{https://yuanhong.larksuite.com/docs/docuseeHRJWAMV3p3uL7yYCOeYx}{Grader} (Password: ZELI)
    \item \href{https://yuanhong.larksuite.com/docs/docussD8ik4yBXShA5kPyRGhgdg}{Worker} (Password: qgF2)
    \item \href{https://yuanhong.larksuite.com/docs/docusfWZk1oYG8qkEMG7y2oxkye}{Web} (Password: Z3Wn)
\end{enumerate}

Following the nomenclature defined in Section~\ref{ch:literature-review-related-work-ai-competition-platforms}, we will call the new system aiVLE 2.0 henceforth. Do note that aiVLE 2.0 also refers to the new system architecture as a whole – every subproject has an independent versioning that does not necessarily share the major revision number 2. This is because components like aiVLE Gym and aiVLE Grader does not exist in aiVLE 1.0.

\section{aiVLE Gym - Separating Agents from Environment}
\label{ch:aivle-gym}
I have released a stable version of aiVLE Gym with all planned features implemented. \href{https://github.com/edu-ai/aivle-gym}{The source} consists of ~1K lines of code, including several example environments and full documentation.

\subsection{Motivation}
aiVLE Gym makes multi-agent competition possible by separating agents from the environment. In a two-agent OpenAI Gym task, we write agent code as shown in Code~\ref{code:two-agent-example}:

\begin{code}
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
env = gym.make("PongDuel-v0") # Two-player Ping Pong game
for ep_i in range(100):
    done_n = [False for _ in range(env.n_agents)]
    ep_reward = 0
    obs_n = env.reset()
    env.render()
    while not all(done_n):
        action_0 = decide_0(obs_n[0], reward_n[0])
        action_1 = decide_1(obs_n[1], reward_n[1])
        action_n = [action_0, action_1]
        obs_n, reward_n, done_n, info = env.step(action_n)
        ep_reward += sum(reward_n)
        env.render()
    print('Episode #{} Reward: {}'.format(ep_i, ep_reward))
env.close()
\end{minted}
\captionof{listing}{OpenAI Gym agent example}
\label{code:two-agent-example}
\end{code}

Note that in a multi-agent scenario, observation, reward and done are all vectors - each element corresponds to one of the agents. Similarly, when you call env.step(), you should provide actions for every agent in this simulation. Such design is acceptable when we perform these multi-agent experiments offline. However, in a competition setting, when it comes to multi-agent tasks, you cannot make decisions for your opponent agents. Therefore, separating agents from the environment simulation is necessary. From the perspective of each agent, it is just like a single-agent environment – the only difference is that the environment is affected by actions taken by other agents as well. Figure~\ref{fig:opanai-gym-multi-arch} and Figure~\ref{fig:aivle-gym-multi-arch} show the architectural differences between OpenAI Gym and aiVLE Gym (each colored box represents a separate process; solid arrows represent inter-process communication):
\begin{figure}[H]
    \centering
    \includegraphics{images/opanai-gym-multi-arch.png}
    \caption{Multi-agent Architecture for OpenAI Gym}
    \label{fig:opanai-gym-multi-arch}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics{images/aivle-gym-multi-arch.png}
    \caption{Multi-agent Architecture for aiVLE Gym}
    \label{fig:aivle-gym-multi-arch}
\end{figure}

\subsection{Design Goal}
Unless mentioned otherwise, all design goals in the following sections are achieved in the released implementation. 

The design goal of aiVLE Gym is to keep full compatibility with OpenAI Gym on the agent side. On the environment side, it lets you convert from existing OpenAI Gym environments with little adaptation. More specifically:

In single-agent case, on the agent side, traditional environment (simulation happens within agent process) and aiVLE environment (simulation happens outside of agent process) should be interchangeable. On the environment side, author can reuse existing OpenAI Gym compatible environment by implementing a serializer that serializes \texttt{action}, \texttt{observation}, and \texttt{info} to JSON compatible objects.

In multi-agent case, on the agent side, the APIs behaves just like normal single-agent OpenAI Gym environment (i.e., interchangeable). On the environment side, author can reuse existing ma-gym~\cite{magym} environment by implementing a serializer along with several metadata fields. 

\subsection{Agent-Environment Communication}
Since agents and environments are separated, there needs to be a communication channel between the two. aiVLE Gym uses a lightweight yet high-performance messaging library ZeroMQ~\cite{zeromq}, which has comprehensive support for many synchronous and asynchronous messaging patterns that are essential to this project. There are two primary challenges for multi-agent tasks when it comes to agent-environment communication:
\begin{enumerate}
    \item The judge should receive and respond to requests asynchronously - it needs to wait for all agents' actions before stepping forward in the environment, then decide what observations/rewards/done to respond to each of the agents.
    \item Certain operations (e.g., reset) must be performed strictly once for each episode, but since each agent will initialize the episode on their own, judge-side will unavoidably receive multiple requests.
\end{enumerate}

To summarize, the judge-side environment needs to implement a "barrier" synchronization mechanism that not only realizes synchronous rendezvous of agent requests, but also performs additional tasks upon the "first-comer" and "last-leaver". 

Therefore, we propose the deterministic finite automaton (DFA) as shown in Figure~\ref{fig:aivle-gym-multi-dfa}:
\begin{figure}[H]
    \centering
    \includegraphics{images/aivle-gym-multi-dfa.png}
    \caption{aiVLE Gym Multi-agent Environment Communication Automaton}
    \label{fig:aivle-gym-multi-dfa}
\end{figure}

This DFA is key to keeping complicated communication details transparent to both agent and environment logic – both can write common synchronous code while the framework deals with the underlying asynchronous logic. Details of this DFA and messaging patterns involved can be found in the \href{https://google.com}{aiVLE Gym appendix}.

\section{aiVLE Grader - Evaluating Agents Using Test Suites}
\label{ch:aivle-grader}
I have released a stable version of aiVLE Grader with 1) normal OpenAI Gym, 2) aiVLE Gym single-agent, and 3) aiVLE Gym multi-agent environment support. The codebase consists of ~600 lines of framework code and ~300 lines of example test suites for all three supported frameworks.

\subsection{Design Goal}
Unlike competitive programming style problems or machine learning prediction tasks, evaluating RL agents is much more complicated than comparing students’ output against a standard answer. With the common programming interface provided by OpenAI/aiVLE Gym, on top of which, we may also create a framework that standardizes/modularizes the initialization, execution, and conclusion of RL agent evaluation. The ultimate goal of this framework, when writing a grader for agents in any OpenAI/aiVLE Gym environment, is to:
\begin{enumerate}
    \item Make the built-in components so complete that for most use cases using built-in ones would be sufficient.
    \item Make each component self-contained without complicated inter-dependencies (i.e., following the single responsibility principle) when writing a custom component.
\end{enumerate}

\subsection{Key Abstractions}
There are three key abstractions to aiVLE Grader: \textit{agent}, \textit{evaluator}, and \textit{test} case.

\textit{Agent} only has two methods: "reset" to reset internal states, "step" to return an action from provided observation. It is flexible enough to allow agents to memorize the history, whilst restrictive enough to prohibit agents from modifying the innerworkings of the environment.

\textit{Evaluator} records the entire execution process and produces a score when the session terminates. It utilizes the common pattern of most RL tasks (see Figure~\ref{fig:obr-loop}): each session consists of many episodes, and each episode consists of many concrete steps. By inserting hook functions to these critical points, an evaluator practically records everything about the evaluation session. 

\textit{Test case} is a bootstrap for evaluation sessions. It wraps \textit{agent}, \textit{environment}, and \textit{evaluator} along with necessary initialization parameters into an object with one simple “evaluate” method. It also offloads certain chore (e.g., runtime limit) away from the user.
\begin{figure}[H]
    \centering
    \includegraphics{images/aivle-grader-class.png}
    \caption{Class Diagram for aiVLE Grader}
    \label{fig:aivle-grader-class}
\end{figure}

\section{aiVLE Worker - Secure and Scalable Grading Client}
\label{ch:aivle-worker}
I have released a stable version of aiVLE Worker with an active CPU-only instance deployed on the server along with aiVLE Web. It is also tested on GPU machines for CUDA compatibility (GTX 1050Ti for CUDA 10, RTX 3070 for CUDA 11). The codebase consists of ~600 LoC, also with examples and full documentation.

\subsection{Design Goal}
As the section title suggests, security and scalability are what aiVLE worker aims to achieve. For security, the implementation is expected to achieve:
\begin{enumerate}
    \item File access restriction: agent program should have no access to directories that may contain sensitive data (e.g., private keys), and should have read only access to files necessary for its execution (e.g., Python binary, dependencies, agent, and environment source code).
    \item Network restriction: agent program should have no access to the Internet. Otherwise, (1) it may benefit from extra computation resources; (2) it may send out confidential runtime details (e.g., configuration of simulation environment) that allow users to fine-tune their program; both of which make the competition unfair.
    \item Resource limit, including RAM limit, CPU affinity (core count) and CPU time limit.
\end{enumerate}

As for scalability, it depends on both the scheduling on the server-side and the execution on the worker-side. In the context of scaling workers easily, a client that requires little permission and setup would be extremely helpful – so that we can deploy the worker on shared GPU servers or even lab machines. What we expect from this solution are:
\begin{enumerate}
    \item Managed concurrency: it should be able to run as many evaluation jobs concurrently as the hardware resource permits, while being able to detect and terminate processes that consume excessive RAM/VRAM. For more details please refer to Section~\ref{ss:aivle-worker-resource-awareness}.
    \item Minimal permission requirement: if we have access to run the submission locally, the environment should be able to operate as a worker node (i.e., sudo is not required).
    \item Minimal dependency requirement: any Linux machine with Python/Virtualenv/Pip should be able to run the worker client.
    \item Moderate overhead: compared to traditional OJ, aiVLE can trade some overhead (both startup and runtime) for absolute essentials like GPU support. However, to achieve a certain level of throughput, crazy warmup time like several minutes is still unacceptable.
\end{enumerate}

\subsection{Security Solution}
For security, I compared three mainstream security solutions: 1) virtual machine (VM) such as Virtualbox, 2) container such as Docker/Podman, and 3) sandbox such as Firejail. The main areas of interest are compared in Table~\ref{tab:security-solutions}:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|cc|c|}
\hline
\multirow{2}{*}{} & \textbf{VM} & \multicolumn{2}{c|}{\textbf{Container}} & \textbf{Sandbox} \\ \cline{2-5} 
 & VirtualBox & \multicolumn{1}{c|}{Docker} & Podman & Firejail \\ \hline
\textbf{Rootless} & No & \multicolumn{1}{c|}{Yes} & Yes & Yes \\ \hline
\textbf{Level of isolation} & Very high & \multicolumn{2}{c|}{High} & Medium \\ \hline
\textbf{Overhead} & High & \multicolumn{2}{c|}{Low} & None \\ \hline
\textbf{Startup time} & $\sim$15s & \multicolumn{2}{c|}{$\sim$3s} & $\sim$0.05s \\ \hline
\textbf{GPU support} & No & \multicolumn{2}{c|}{Yes, with NVIDIA container runtime} & Yes \\ \hline
\end{tabular}
\caption{Comparison of Mainstream Security Solutions}
\label{tab:security-solutions}
\end{table}


There are several must-haves for the candidate solution:

\begin{enumerate}
    \item GPU-support. Many recent RL algorithms are practically impossible to run without a GPU. This eliminates VM without PCI passthrough.
    \item Server needs to be shared. This eliminates VM with PCI passthrough as it requires exclusive access. This also eliminates Podman and Docker, as it prevents the GPU from being shared by any other container with root access.
\end{enumerate}

Therefore, Firejail is the only option left. I built a custom security profile to expose only the necessary file system and devices to processes inside the sandbox. I also used Firejail to impose CPU affinity and RAM usage limit on each sandbox.

\subsection{Resource Awareness}
\label{ss:aivle-worker-resource-awareness}
\subsubsection{Resource Monitoring - \texttt{monitor} module}
\label{sss:monitor}
To achieve resource-sensitive load balancing as described in aiVLE Web section, we need to monitor CPU/GPU utilization and RAM/VRAM usage in real-time and control the worker's subscription to the task queue accordingly in real time. Therefore we implement the \texttt{monitor} module inside aiVLE Worker that runs in parallel with the main worker process, which
\begin{enumerate}
    \item Monitors the system utilization periodically
    \item Control the worker's task queue subscription according to prefetched threshold
    \item Send monitoring metrics to the \texttt{warden} module to enable resource limit enforcement
\end{enumerate}

The inter-module/process communication will be discussed shortly in Section~\ref{sss:warden}. The first two objectives can be achieved by the execution flow illustrated in Figure~\ref{fig:aivle-worker-monitor-flow} with the help of \href{https://pypi.org/project/psutil/}{\texttt{psutil}} for CPU/RAM monitoring and \href{https://github.com/fbcotter/py3nvml}{\texttt{py3nvml}} for GPU monitoring.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/aivle-worker-monitor-flow.png}
    \caption{Execution Flow of \texttt{monitor} module}
    \label{fig:aivle-worker-monitor-flow}
\end{figure}

\subsubsection{Limit Enforcement - \texttt{warden} module}
\label{sss:warden}
Celery does not allow the worker itself to stop consuming tasks - the worker can only choose to shutdown itself entirely. And the \texttt{monitor} module is periodically checking the system load, so we need to have the \texttt{warden} module that is responsible for enforcing the resource limits by terminating the job externally and report the reason to aiVLE Web.

There are three modules that run in parallel: worker, monitor and warden. And they need to exchange information in order to collaborate. This is where ZeroMQ~\cite{zeromq} comes handy (again): only the worker knows the mapping from sandbox PID to task information (i.e., VRAM limit), and only the monitor knows the VRAM usage of each process. Both need to send their data to the warden process which oversees all running jobs and terminates jobs that violates restrictions. Figure~\ref{fig:aivle-worker-ipc} shows the inter-process communication among these three:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/aivle-worker-ipc.png}
    \caption{aiVLE Worker Inter-process Communication}
    \label{fig:aivle-worker-ipc}
\end{figure}

For \texttt{warden}'s case, it receives:
\begin{enumerate}
    \item (from worker) mapping from sandbox PID to corresponding task information
    \item (from monitor) VRAM usage of each process
\end{enumerate}
Every time it receives an update from the monitor, it goes through the list of active sandboxes, figuring out process hierarchy\footnote{This is necessary as most frameworks like PyTorch will spawn new processes for calculation. Generally the process that is utilizing GPU is no longer the parent process. And there might be multiple processes in the same sandbox that consumes VRAM.} to calculate the total VRAM usage of each sandbox, and finally checks if any sandbox needs to be terminated.


\section{aiVLE Web - AI competition platform}
\label{ch:aivle-web}
aiVLE Web consists of a Django backend that is ~2K LoC and a React frontend that is ~2K LoC. An active instance is deployed on the server and is accessible from \href{https://aivle.leotan.cn/}{https://aivle.leotan.cn/} (frontend) and \href{https://aivle-api.leotan.cn/}{https://aivle-api.leotan.cn/} (API and admin panel).

\subsection{Design Goal}
There are three primary considerations to the design of aiVLE web: extensibility, scalability, and usability. 

Extensibility means the architecture should be flexible enough for future upgrades including but not limited to multi-agent task competition and real-time match spectate. Scalability means the platform should be able to spread the evaluation workload, which is obviously the most computationally heavy task, over hundreds of worker machines. Usability means the platform should provide all the features the users (e.g., CS4246 teaching team) need for a successful semester of teaching.

\subsection{Highly Available and Fault Tolerant Task Queue}
\label{ch:aivle-web_highly-available-task-queue}
This is a continuation of scalability of (grading) workers. In the section for aiVLE worker, we addressed the problem of running the worker on as many computers as possible with as little configuration/permission as possible. Here we address the problem of 1) coordinating the communication between the workers and the backend server, and 2) distributing grading tasks to the workers efficiently for shorter waiting time and higher resource utilization.
\subsubsection{The Problem}
In original aiVLE, there was no concept of "task queue". All pending tasks are stored in the DB with the status "QUEUED". Communication between the worker (or runner as per the term used by original author) and server is \textit{half-duplex by polling}. In other words, the worker makes \textbf{periodic} requests to the server for new ungraded submissions:
\begin{figure}[H]
    \centering
    \includegraphics{images/aivle-web-old-task-queue.png}
    \caption{Old aiVLE “Task Queue”}
    \label{fig:aivle-web-old-task-queue}
\end{figure}
There are three critical limitations to this approach:
\begin{enumerate}
    \item Worker-side polling doesn't scale well: there is zero mechanism in orchestrating the timing and order of workers polling the server for new jobs. The severity of possible traffic spike (i.e., many workers poll the server at the same time due to lack of coordination) increases linearly with the number of worker nodes.
    \item Possible race condition: if two worker pull submissions at the same time, both will get the same ungraded submission. Such redundant work will get more significant with more worker nodes, which hurts the overall efficiency of the worker cluster.
    \item No concurrency on each worker: each individual worker only polls for new job when it has no submission to grade. In other words, each worker can only grade one submission at a time.
    \item No load balancing: the backend has no control over which worker grades which submission, therefore load balancing is virtually impossible
\end{enumerate}

\subsubsection{The Solution}
Similar to the idea of extracting the responsibility of data storage/management into a separate database backend, we delegate the messaging tasks to a message queue. Conceptually, message queue enables asynchronous communication between clients (who submit tasks) and workers (who finish tasks). Below is a diagram illustrating how the MQ-based task queue works:
\begin{figure}[H]
    \centering
    \includegraphics{images/aivle-web-mq.png}
    \caption{Message Queue Based Task Queue}
    \label{fig:aivle-web-mq}
\end{figure}
Every worker listens to one or more "queues", where the message broker is responsible for allocating tasks fairly among workers in each queue. When an evaluation job comes, aiVLE backend will submit the job to an appropriate queue (i.e., private queue if user has dedicated workers available, otherwise public CPU/GPU queue according to task specification) and wait for the assigned worker to submit evaluation result. The task will remain in the queue until it is processed (i.e., message queue is persistent). A randomly generated task ID is used to authenticate worker's submission – only the worker whom the broker assigned the task will have this ID. This approach not only reduces the number of requests to be $O(n)$ where $n$ is the number of evaluation jobs, but also ensures fair assignment of tasks among workers. Moreover, we also benefit from other standard features of MQ such as automatic retry and heartbeat checks.

\subsection{Resource-sensitive Load Balancing}
\label{ss:aivle-web-load-balancing}
By using message queue for task distribution, we already have some primitive load balancing - Celery with RabbitMQ backend by default dispatches messages to all consumers in round-robin style, therefore all consumers are expected to consume the same number of tasks from the same queue over a fixed period of time. Although this is a huge improvement over no load balancing at all, from some stress testing using real-world cases, we find it necessary to take system load into consideration. Specifically, the primitive method of load balancing by number of tasks works poorly when certain tasks are much more resource intensive. For example, both worker A and worker B receive 5 submissions, but the ones for worker A consumes 30\% of total VRAM while the ones for worker B takes only 10\% of total VRAM. There are two serious implications in this imaginary scenario:
\begin{enumerate}
    \item \textbf{Unfairness}: suppose the worker is configured to run at most 8 concurrent evaluations, the fourth and fifth submissions arriving at worker A will not have sufficient VRAM. They will likely receive runtime errors which are entirely ours to blame.
    \item \textbf{Inefficiency}: suppose the task queue is configured to automatically retry the failed job by putting a new evaluation job into the same queue, since worker A is still accepting submissions, the retry attempt may take additional fails to finally arrive at worker B.
\end{enumerate}

The key to solving such unwanted behavior is 1) to monitor the available system resources, 2) to stop processing new tasks when the available resources fall below certain thresholds. Most of the heavy-lifting is handled on the worker side, for aiVLE Web, it merely uses \texttt{add\_consumer} and \texttt{cancel\_consumer} Celery APIs for resuming and pausing consuming respectively. There are two points to note in this solution:

\begin{enumerate}
    \item It does not cancel already running jobs - it only stops receiving new tasks when the threshold is met. This behavior is acceptable in our case since we only need to promise students with a certain amount of resources to run their submissions. And our solution guarantees\footnote{Technically speaking this guarantee is not true between two checks on system information, but we can easily reduce the impact by performing the check more frequently. In our experience, one second interval is more than enough.} the promised amount of CPU/RAM/VRAM at the beginning of any evaluation job.
    \item It does not balance the resource utilization among all workers. \texttt{cancel\_consumer} simply removes the worker from the list of consumers to dispatch tasks to. To achieve resource utlization balance, aiVLE Web needs to understand each worker's utlization in real-time and dispatch tasks accordingly. We think the overhead and complexity of this design significantly outweights the potential benefits.
\end{enumerate}

\subsection{Role-based Permission Model}
\label{ss:aivle-web-permission-model}
In Django we have two major types of permission model: per-model permission (built-in authentication system) and per-object permission (\href{https://github.com/django-guardian/django-guardian}{django-guardian}). Per-model permission means the finest granularity is model-level, meaning we may only check if the user has read/write/edit/delete access to the model. Per-object permission is the finest granularity possible as we can establish arbitrary permission between any object and any user. In fact, the worst possible space complexity is $O(mnk)$ where $m$ is the number of objects, $n$ is the number of users and $k$ is the number of permissions.

In our use case, model-based permission is too coarse-grained: a student should only be able to view the tasks in the course he/she enrolled in, rather than all the tasks available on the platform. On the other hand, object-based permission is too overkill as well: there might be hundreds of thousands of job records in the database, storing each user's permission to each job record is simply too wasteful\footnote{This can be avoided by properly configuring django-guardian, but its flexibility does allow arbitrary permission on any object as mentioned earlier. In fact, if we restrict django-guardian to avoid such wasteful behavior, we essentially have the soon-to-be-discussed role-based permission model.}. Therefore, we propose a sweet spot between the two extremes: role-based permission model.

The entity relationship diagram of aiVLE Web is illustrated in Figure~\ref{fig:aivle-web-er-diagram}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/aivle-web-er-diagram.png}
    \caption{aiVLE Web Entity Relationship Diagram}
    \label{fig:aivle-web-er-diagram}
\end{figure}

In the relational entity Participation, we record the "role" of the User in a Course. Possible roles\footnote{Note that a user can have different roles in different courses. And superuser who has access to absolutely everything is out of discussion here.} (as defined in \texttt{app.model.participation}) are: admin (ADM), lecturer (LEC), teaching assistant (TA), student (STU), guest (GUE).

The idea of role-based permission model is centered around one's role in the corresponding course: to determine one's access to any object, we first find the object's related course, then check if the user has access to this object in the context of this related course. For example, if we need to know if the user has view access to a certain Job, we can follow the arrows in the diagram: Job $\to$ Submission $\to$ Task $\to$ Course $\to$ Participation $\to$ User to find the corresponding role\footnote{The utility function \texttt{has\_perm} automatically queries the role given the course and the user, so its argument is the course instead of the role.}, and check if this role has \texttt{job.view} permission according to the permission lookup table (see Table~\ref{tab:aivle-web-permission-table}).

By introducing the Participation relational entity and \texttt{has\_perm(course, user, permission)}, we compressed the worst case $O(mnk)$ space complexity to $O(nk)$ and it is a huge improvement. In reality the number of objects $m$ significantly outweighs the number of users $n$ or permissions $k$. We can summarize different roles' access (as defined in aiVLE.settings.ROLES) using a permission matrix in Table~\ref{tab:aivle-web-permission-table}:

\begin{table}[H]
\centering
\begin{tabular}{|c|l|l|l|l|l|l|}
\hline
\multicolumn{1}{|l|}{} &  & Admin & Lecturer & TA & Student & Guest \\ \hline
\multirow{5}{*}{Task} & View opened tasks & x & x & x & x & x \\ \cline{2-7} 
 & View all tasks & x & x & x &  &  \\ \cline{2-7} 
 & Add task & x & x &  &  &  \\ \cline{2-7} 
 & Edit task & x & x & x &  &  \\ \cline{2-7} 
 & Delete task & x & x &  &  &  \\ \hline
\multirow{4}{*}{Submission} & View own submissions & x & x & x & x &  \\ \cline{2-7} 
 & View all submissions & x & x & x &  &  \\ \cline{2-7} 
 & Add submission (under own name) & x & x & x & x &  \\ \cline{2-7} 
 & Download submission & x & x & x &  &  \\ \hline
\end{tabular}
\caption{(Partial) Permission Lookup Table}
\label{tab:aivle-web-permission-table}
\end{table}