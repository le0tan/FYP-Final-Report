\chapter{Conclusion}
\label{ch:conclusion}

\section{Summary}
\label{s:conclusion-summary}
We started this project in pursuit of a complete solution for RL algorithm evaluation that is extensible, scalable, and easy-to-use. We think these three objectives are critical for the success of such a platform, the lack of which on existing solutions motivated us to build one that is up to our standard.
During the two semester of this final year project, we
\begin{itemize}
    \item Implemented RL environment framework that supports agent-environment separation and multi-agent competition.
    \item Provided automated and secure evaluation for RL tasks.
    \item Design and deployed distributed evaluation subsystem based on message queue.
    \item Improved the web application for hosting competitions by 1) restructuring code, 2) comprehensive documentation, 3) adding new features (e.g., email verification, password reset, invitation token, course whitelist, etc.).
\end{itemize}

For the RL environment framework, we have invited another FYP student Ho Hol Yin for an early adoption in his project ``A unified testbed for AI teaching and research''. The feedback has been positive especially in terms of OpenAI Gym compatibility.

For the evaluation subsystem, we deployed the system to SoC compute cluster during the winter break and performed several performance experiments. We have validated the fair distribution of evaluation jobs, achieved $\sim$3 times of utilization improvement over the previous solution and demonstrated the new system’s scaling capability of supporting CS4246.

For the competition-hosting web platform, by working with Dr. Narayan closely, we have addressed most of the pain points from CS4246 teaching team. More importantly, the documentation is now sufficient for fresh deployments and future upgrades/maintenance.

\section{Limitations}
\label{s:conclusion-limitations}
While we think this project has been going generally smoothly, and the outcome is satisfactory, we must admit the limitations of the work done:

First, as mentioned in Section~\ref{ss:deployment-limitations}, due to limited resources, we could not test aiVLE 2.0's scalability on more machines (say dozens or even hundreds of nodes). Once we put aiVLE 2.0 into CS4246 production environment, we wish to onboard more stakeholders so that we can gradually scale up the system and explore its true potentials.

Second, aiVLE 2.0 does not provide a production-ready multi-agent competition solution. We started strong with aiVLE Gym that works great for multi-agent competitions, but once we try to extend aiVLE Web to support multi-agent tasks, we encountered much more difficulties than we anticipated. Most notably, we found it challenging to implement a skill rating and matchmaking\footnote{Here ``match’’ is equivalent to competition. ``Matchmaking'' means making matches that having the outcome of which significantly reduces our uncertainty of the competitors' skill ratings. Having similar skill ratings is only one aspect of matchmaking: the system gains little new knowledge by having the same pair compete against each other repeatedly, even if they have very similar skill ratings.} system that balances efficiency and fairness. Nevertheless, we managed to implement the infrastructure and abstractions for the matchmaking system (see Appendix~\ref{appendix:aivle-web_matchmaking}), and we definitely hope to materalize the proposal by implementing actual matchmaking algorithms on the infrastructure.

Lastly, although I emphasized the importance of maintainability repeatedly in this project by adopting many software engineering best practices such as comprehensive documentation and maintainable architecture, there is still some room for improvement, especially on the frontend code – unlike backend development that I am familiar with, this project is my first serious attempt with frontend development. Looking at the codebase after a year of experience, we think the frontend code needs considerable reorganization to achieve similar level of maintainability as other codebases.

\section{Future Work}
\label{s:conclusion-future_work}
Instead of calling aiVLE 2.0 a finished project, we think it is more appropriate to call it a foundation\footnote{...that is built upon the foundation of aiVLE 1.0} of a truly comprehensive AI competition platform. Our efforts in improving the project's maintainability are not only for the operation of CS4246 assignment grading, but also for the future upgrades from whoever interested in making it even more advanced.

In particular, as mentioned in Section~\ref{s:conclusion-limitations}, a concrete implementation of multi-agent matchmaking and skill rating system would be a huge upgrade - it opens up the possibility of multi-agent tasks that could be more interactive and competitive.

For the evaluation subsystem, we feel there is still room for improvement w.r.t. load balancing strategy - currently we distribute tasks fairly by round-robin, and pauses assigning new jobs to nodes that are under pressure. However, if we managed to solve the significant communication overhead of reporting utilization data to the master server in real-time, we could implement some load balancing algorithm that balances the system load among the workers.

And of course, as mentioned in Chapter~\ref{ch:design-and-impl}, aiVLE Gym is designed to function independently outside of aiVLE platform. Its capability of separating environment from agents could be useful for academic research in, say, human-in-the-loop machine learning.
