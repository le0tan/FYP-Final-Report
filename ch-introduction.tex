\chapter{Introduction}

With the increasing popularity in both artificial intelligence (AI) research and industry application, more and more institutions are considering AI education as an integral part of their computer science or even general education curriculum. However, in spite the algorithmic nature of AI education, there is surprisingly few platforms and tools to host AI competitions/assignments and automatically grade the submissions. Unlike traditional algorithm and data structure courses that have many online judges available (to name a few, \href{https://codeforces.com/}{Codeforces}, \href{https://open.kattis.com/}{Kattis}, \href{https://onlinejudge.org/}{UVa Online Judge}), AI courses often rely on arbitrary grading scripts or in-house solutions to host their assignments/contests. There are well-known data science competition platforms like \href{https://www.kaggle.com/}{Kaggle} that are suitable for prediction/classification tasks, but the area of reinforcement learning (RL) tasks \footnote{To be precise, here tasks mean environments that are typically used for RL algorithms. The differentiating characteristic of such tasks is that they are interactive (in contrast to comparing the output against the ground truth like what Kaggle and other traditional OJs do).} remains surprisingly untouched. Therefore, this project tries to fill the gap of evaluating algorithms that solve interactive tasks. We aim to provide a complete solution from building RL \footnote{Technically the evaluation model we use is suitable for almost all AI/ML tasks (including classification and regression), but we will focus on RL first since we already have Kaggle for classification/regression tasks.} environments, writing test cases, to eventually hosting these tasks on a massively scalable platform.
