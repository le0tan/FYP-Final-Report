\chapter{Introduction}

With the increasing popularity in both artificial intelligence (AI) research and industry application, more and more institutions are considering AI education as an integral part of their computer science or even general education curriculum. However, in spite of the algorithmic nature of AI education, there are surprisingly few platforms and tools to host AI competitions/assignments and automatically grade the submissions. Unlike traditional algorithms and data structure courses that have many online judges available (to name a few, \href{https://codeforces.com/}{Codeforces}, \href{https://open.kattis.com/}{Kattis}, \href{https://onlinejudge.org/}{UVa Online Judge}), AI courses often rely on arbitrary grading scripts or in-house solutions to host their assignments/contests. There are well-known data science competition platforms like \href{https://www.kaggle.com/}{Kaggle} that are suitable for prediction/classification tasks, but the area of reinforcement learning (RL) tasks\footnote{To be precise, here tasks mean environments that are typically used for RL algorithms. The differentiating characteristic of such tasks is that they are interactive (in contrast to comparing the output against the ground truth like what Kaggle and other traditional online judges do).} remains surprisingly untouched. Therefore, this project tries to fill the gap of evaluating algorithms that solve interactive tasks. We aim to provide a complete solution from building RL\footnote{The evaluation model we use is suitable for almost all AI/ML tasks (including classification and regression), but we will focus on RL in this case.} environments, writing test cases, to eventually hosting these tasks on a massively scalable platform.

\section{Background}
\subsection{Reinforcement Learning Tasks}
Reinforcement learning (RL) is a branch of machine learning that maps observations (possibly with history as well) to actions such that certain metrics are maximized~\parencite{sutton-barto}. The metrics which agents try to maximize are called a reward or reinforcement, thus the name “reinforcement” learning. In a broader sense, any AI task is RL task: we can generalize any input as the observation, any output as the action and any target as the reward. But for the sake of simplicity, in this report, by RL task we mean sequential decision problems (SDP), where the outcome of agent’s decision depends on a sequence of decisions~\parencite{russell}.

A sequential decision problem is fully described by state space, action space, transition model and a reward function~\parencite{russell}. A transition model is a function that maps (current state, history states, action) to a probability of reaching any possible state. If the transition model does not depend on history state, we call it Markovian as the probability only depends on the current state instead of the history of all earlier states. A reward function maps current state\footnote{Reward function may also depend on action and next state, but one that only depends on the current state is the most common variant, and this difference is not fundamental.} to a numerical reward.

The solution to a sequential decision problem is called a policy. It is a function that maps current state (and sometimes history states) to an action. We always assume that the domain of the policy function is the entire observation space so that the agent knows what action to take under any possible circumstance. The goal of RL algorithms is to come up with such policy functions that maximize their expected reward under different settings (e.g., unknown underlying transition model, partial observability, etc.).

From the definition of sequential decision problems, we can summarize the following execution loop of any typical SDP:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{images/obr-loop.png}
    \caption{Action-observation-reward Loop}
    \label{fig:obr-loop}
\end{figure}

In other words, the environment implements the transition model $P\left(s_j\middle| s_i,a\right)$ and reward function $r\left(s_i\right)$ and returns the next observation $o_j$ and reward $r_j$ to the agent, after which the agent takes another step of action $a_{j+1}$ based on $\left[o_j,o_{j-1},\cdots,o_0\right]$ and $\left[r_j,r_{j-1},\cdots,r_0\right]$. Note that $o_i$ and $s_i$ may or may not be the same depending on whether the environment is fully observable.

\section{Related Work}

\subsection{Reinforcement Learning Environment Frameworks And Collections}
RL research, like many computer science topics, requires concretizing the theory with implementation and experiments as they are equally important. That is where RL environment frameworks and collections like the Arcade Learning Environment (ALE)~\parencite{ale} and OpenAI Gym~\parencite{openai-gym} come into play. Such an effort is incredibly important for the rapid growth of the RL community: it not only saves researchers a lot of emulation development time, but also provides benchmark environments to evaluate RL tasks similar to how CIFAR-10~\parencite{cifar-10} provides benchmark datasets in image classification.

Besides collecting many classical environments as a benchmark package, an important contribution of OpenAI Gym is its widely accepted application programming interface (API). This common interface (see Code~\ref{code:gym-agent}) proves to be sufficient for not only hundreds of self-contained environments included in OpenAI Gym, but also many more third-party environments. When it comes to turning retro video games to RL environments, ALE eventually adopted Gym's API and merge into Gym. Moreover, Gym Retro \parencite{gym-retro} adds even more emulated systems such as Nintendo Game Boy and Sega Genesis to the Gym ecosystem. Therefore, it is safe to say that following this interface gives us enough flexibility to implement most of the desired environments.

From the source code of OpenAI Gym, a compatible environment needs to implement the methods and attributes shown in Figure~\ref{fig:gym-class}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/gym-class.png}
    \caption{OpenAI Gym Class Diagram}
    \label{fig:gym-class}
\end{figure}

These abstractions (i.e., \texttt{action\_space}, \texttt{observation\_space} and \texttt{step(action)}) are in line with the academic definition of sequential decision problems (SDP). Thus, in the following example of a typical OpenAI Gym agent program (Code~\ref{code:gym-agent}), there is also a loop structure similar to the action-observation-reward loop in SDPs (Figure~\ref{fig:openai-gym-obr-loop}):

\begin{code}
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=LightGray,fontsize=\footnotesize,linenos,samepage,highlightlines={6-8}]{python}
import gym

env = gym.make('CartPole-v0')
for i_episode in range(100):
    observation = env.reset()
    for t in range(100):
        action = decide(observation, reward)
        observation, reward, done, info = env.step(action)
env.close()
\end{minted}
\captionof{listing}{OpenAI Gym Agent Example}
\label{code:gym-agent}
\end{code}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/openai-gym-obr-loop.png}
    \caption{Action-observation-reward Loop in OpenAI Gym}
    \label{fig:openai-gym-obr-loop}
\end{figure}

Since this project is supposed to be designed for future multi-agent task support from ground up, we also looked into the multi-agent environment support of OpenAI Gym: although authors of OpenAI Gym listed it as a future direction when they published the paper in 2016, as of 2022, there is still no official support for multi-agent tasks. There are some third-party environments that support multi-agent tasks, like ma-gym \parencite{magym}. These implementations usually convert multi-agent observation, reward, and action into a vector, length of which is equal to the number of participating agents in the environment.

\subsection{Online Judging System}
Online judges first emerged to address the problem of automatically grading programming assignments conveniently, fairly and securely \parencite{RN4}. Without any manual intervention after the system is configured properly, online judge systems need to handle 1) receiving submissions, 2) compiling and executing source codes, 3) comparing outputs against the correct answers, and 4) logging the result. For fairness, RAM and CPU usage will be monitored and limited. Usually, there is a hard time limit after which any submission will be forced to terminate. For security, execution of arbitrary submission needs to be contained in a safe environment with heavily restricted privilege levels such that only necessary operations are allowed. Any other attempt (e.g., accessing the network, read/write on filesystem, etc.) will be blocked and preferably reported to the system administrator. 

Among all the challenges in building a successful online judge, security is probably the most technically difficult aspect. Some online judges were used in an exam setting where students had to use dedicated terminals for access, and a system administrator is involved in monitoring the security issues \parencite{10.1145/384267.305835}. For automated security measures, there are generally two approaches: 1) source code or binary scanning, and 2) operating system level access control. 

Source code or binary scanning has no runtime performance penalty, but it requires an exhaustive check on all possible malicious tokens/system calls. It is unrealistic to have a perfectly secure source code scanner and every supported language requiring a new set of scanning rules makes this approach non-sustainable \parencite{RN4}. Besides, it is impossible to scan the binary for programs written in a dynamic programming language such as Python as they execute inside an interpreter.

Access control based on operating system kernel security features is much more foolproof and widely applicable. Such techniques are also called OS-level virtualization because from the perspective of programs inside a ``namespace'', it can only see partial filesystem and devices that are assigned to this “namespace”. Since the introduction of user namespaces in Linux kernel 3.8, containerization technology has become increasingly popular to provide isolation between processes while being able to share nearly all hardware resources with minimal performance overhead \parencite{RN16}. 

With the rapid growth of cloud and distributed computing, there are also novel online judges that attempt to adopt these cutting-edge technologies. A distributed online judge architecture tries to address the problem of scalability: an online judge needs to execute student submissions, and with more students and courses, computation required grows linearly. It is difficult to scale vertically (making computers more powerful) but easy to scale horizontally (having more computers) especially in recent years with the emergence of cloud computing \parencite{RN17}. Evaluating AI tasks has an even stronger demand for horizontal scalability as neural networks or sophisticated searching algorithms are notoriously computationally heavy. Notable implementations of distributed online judge include MetaOJ~\parencite{metaoj}, which separates data storage, web application and judgers into three separate components that can be deployed to the cloud independently (Figure~\ref{fig:metaoj}).

\begin{figure}[H]
    \centering
    \includegraphics{images/metaoj.png}
    \caption{MetaOJ Architecture \parencite{metaoj}}
    \label{fig:metaoj}
\end{figure}

\subsection{ML/RL Benchmark Platforms}
\label{ss:intro-rl-benchmark-platforms}
In contrast to competition platforms on the education side, researchers are more interested in a benchmark platform that provides a collection of tasks to compare different RL algorithms. Such benchmarks are crucial for reproducible and verifiable research in RL. According to~\textcite{RN20}, an interpretable and reproducible experiment consists of four parts: algorithm, parameters, evaluation method and dataset. For a benchmark, evaluation method and dataset are fixed while contributors will compare different algorithms and parameters. In the context of RL benchmark, this means packaging the simulation environment and evaluation metrics behind a simple set of common interfaces for both training and testing.

However, as machine learning models (along with their training processes) get much more sophisticated, reproducibility is affected by not only those four points – random seeds, version of external libraries and even specific model of CPU/GPU could greatly affect the result of the same code \parencite{RN21}. Therefore, according to “Ten Simple Rules for Reproducible Computational Research” by \parencite{RN22}, for an RL benchmark platform to be reproducible, it should also: 1) have strict versioning\footnote{Meaning “any” change to the implementation of either environment or evaluation metrics should be reflected in a different version number. This also includes changes to the version of external dependencies.} of environments and evaluation metrics; 2) use deterministic methods whenever possible and fix random seeds throughout the process; and 3) have a record of all raw data.

Considering the criteria listed above, OpenAI Gym\parencite{openai-gym} is possibly one of the closest to an ideal benchmark: it explicitly embraces strict versioning, has an interface to fix random seeds, and includes many well-accepted tasks. Unfortunately, it only handles the simulation side of RL benchmark while leaving the evaluation to the users. In comparison, for a benchmark \textbf{platform} like Kaggle to succeed, it needs to not only provide dataset (corresponding to simulation environment in RL), but also enforce common scoring criteria (corresponding to evaluation metrics in RL). Thus, providing a framework for lecturers and researchers to easily apply the common scoring criteria is worth considering for a future RL benchmark \textbf{platform}.

\subsection{AI Competition Platforms}
\label{ch:literature-review-related-work-ai-competition-platforms}
aiVLE (AI Virtual Learning Environment) is the inspiration and foundation of this project. It is an RL task evaluation system built by the CS4246 teaching team since 2019. We will call the old aiVLE as aiVLE 1.0 henceforth\footnote{Do note that aiVLE 1.0 refers to the system as a whole –- it does not refer to any specific component (e.g., web, runner, runner-kit).}. aiVLE 1.0 addresses the need for evaluating an OpenAI Gym agent in a GPU-accelerated environment. The architecture of aiVLE 1.0 is illustrated in Figure~\ref{fig:aivle-1-arch}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/aivle_1_arch.png}
    \caption{Architecture of aiVLE 1.0}
    \label{fig:aivle-1-arch}
\end{figure}

Due to urgency and time constraints, the author of aiVLE 1.0 prioritized “making it work” over “doing it right”, which leaves us four pain points\footnote{The list here is not exhaustive: more limitations and considerations will be discussed in Implementation Progress section, along with solutions to these pain points.} that impact its \textbf{extensibility}:

\begin{enumerate}
    \item Lack of documentation. For example, 1) deployment documentation for the runner instances (programs that evaluate student submissions), 2) architecture overview of each component and the entire system, and 3) inline documentation such as function usage, class definition. Without these documentation, new developers and users will take a much longer time to adopt and contribute to the system. 
    \item Need more coverage of software engineering best practices such as proper level of abstraction, avoiding lengthy functions/files for interpretability, etc.
    \item No frontend-backend separation. Navigating and understanding the codebase is difficult as there is no clear boundary between frontend and backend code. Moreover, using JavaScript for frontend interactivity would make the codebase look even more convoluted.
    \item No multi-agent support.
\end{enumerate}

As for \textbf{scalability}, lack of proper task queue makes it difficult to scale aiVLE 1.0 to more machines: its workers get evaluation tasks by requesting server for the latest submissions \textit{periodically}. Worker-side polling has risks for traffic congestion and race condition, and the consequent lack of load balancing further hurts aiVLE 1.0’s potential to scale well with more workers. More details on the task queue and load balancing mechanism are covered in Section~\ref{ch:aivle-web_highly-available-task-queue} and Section~\ref{ss:aivle-web-load-balancing}.