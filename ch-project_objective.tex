\chapter{Project Objective}
\label{ch:project-objective}
\section{Problem Statement}
\label{s:project-objective-problem-statement}
The goal of this project is to provide a complete solution for RL algorithm evaluation that is extensible, scalable, and easy-to-use. It consists of four tightly integrated components (collectively called aiVLE 2.0):

\begin{enumerate}
    \item aiVLE\footnote{aiVLE is name for the grading system currently used in CS4246, more details about the old aiVLE (aiVLE 1.0) are covered in Section~\ref{ch:literature-review-related-work-ai-competition-platforms}} Gym: An OpenAI Gym \cite{openai-gym} compatible RL environment framework with agent-environment separation and official support for multi-agent tasks.
    \item aiVLE Grader: An auto grading framework for aiVLE Gym tasks.
    \item aiVLE Worker: A security sandbox to execute arbitrary code submissions safely in a controlled environment, and a massively scalable worker client for evaluation.
    \item aiVLE Web: A web application for hosting RL competitions.
\end{enumerate}

\section{Motivation}
\label{s:project-objective-motivation}
Courses like CS4246, which teach AI, and are different from traditional programming/algorithm courses, need a system to automate the process of collecting and evaluating programming assignments on AI tasks. aiVLE (AI Virtual Learning Environment) is the inspiration and foundation of this project – it is an RL task evaluation system built by the CS4246 teaching team since 2019. We will call the old aiVLE as aiVLE 1.0 henceforth\footnote{Do note that aiVLE 1.0 refers to the system as a whole –- it does not refer to any specific component (e.g., web, runner, runner-kit).}. aiVLE 1.0 provides instant feedback on programming assignments that require varied computational power such as GPU or other specialized processing units. However, aiVLE 1.0 has many pain points. For example, it lacks extensibility (e.g., does not support multi-agent task), scalability (e.g., no concurrency safety for many workers), and documentation for more courses to make use of the platform. As for another similar platform called Botzone~\cite{botzone}, although it’s built for external users, limitations like no GPU support and no common interface make it infeasible for many tasks.

Secondly, an open source RL competition platform with good documentation and software engineering best practices has potential impact beyond education purposes. On the one hand, such a platform could also be useful for benchmarking AI algorithms – the consistency required for grading assignments is perfect for comparing research as well. On the other hand, examples like AlphaGo~\cite{alphago} proves the effectiveness of combining supervised learning from past match data with unsupervised RL – match history collected on the platform could be useful for training models for corresponding tasks. 

Lastly, there is demand for such a platform. Both CS3243 and CS4246 have assignments on RL algorithms. And even for many non-RL chapters such as brute force or informed search, satisfiability problems, the techniques could also be used to solve RL tasks. It is a safe bet that such a platform will benefit many AI courses (including CS2109S and CS3244) by using it alongside platforms like Kaggle to cover most AI algorithm evaluation scenarios.

\section{Roadmap}
\label{s:project-objective-roadmap}
There are three stages planned for this project. During the first semester, we delivered satisfactory results for the first two stages. During the winter break, we conducted \hyperref[ch:deployment-and-testing]{real-world deployment and testing} to evaluate the feasibility of advancing the project to the third stage. During the second semester, we focus on fixing problems found during the test, and delivering features as mentioned in the third stage.

\subsection{Stage 1: Framework}
Before developing the web platform for hosting the competitions, we first need frameworks for 1) creating environment and 2) evaluating agent’s performance – \hyperref[ch:aivle-gym]{aiVLE Gym} and \hyperref[ch:aivle-grader]{Grader} respectively. These frameworks are independent from the web platform, therefore could be used separately for other education or research purposes as well.
Besides platform-independent frameworks, a security sandbox, \hyperref[ch:aivle-worker]{aiVLE Worker}, is also necessary for hosting competitions. These three frameworks will be implemented in the mentioned order as the latter have dependency on the former.

\subsection{Stage 2: Platform}
With the frameworks ready, the second stage is to have \hyperref[ch:aivle-web]{a web platform} that achieves basic online judge functionality, which includes but is not limited to: 1) user registration and authentication, 2) creating/joining courses, 3) creating/modifying/submitting to tasks, 4) showing the evaluation result. The target of this stage is to have a \emph{minimum viable product} that CS4246 can use to host single-agent assignments with GPU-accelerated evaluation.

\subsection{Stage 3: Advanced Solution}
Stage 1 and 2 provide a solid foundation for further extension and exploration, but primarily focus on delivering features that are necessary (thus \textbf{minimally} viable) for CS4246 teaching. Stage 3 aims to further extend the use case of our web platform. First, we will improve the documentation of the source code and use cases, which is crucial for the maintainability and upgradability of this project. Second, we will add advanced features that are so-called nice-to-have\footnote{There are also many other nice-to-have features that did not appear in this report, like more user-friendly frontend and course administration tools. To keep the report relatively concise, features that do get covered are either technically challenging or ``interesting''.} (e.g., Sections \ref{ss:aivle-worker-resource-awareness}, \ref{ss:aivle-web-load-balancing}) but are achievable within the time constraint. Lastly, we may explore the application of this project in terms of teaching multi-agent algorithms, human-in-the-loop computing and even try building a test bed for RL algorithms and host the benchmark on the same platform for researchers\footnote{``A unified testbed for AI teaching and research'' by Ho Hol Yin (Project ID H247060)}.