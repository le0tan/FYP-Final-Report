\chapter{Project Objective}
\label{ch:project-objective}
\section{Problem Statement}
\label{s:project-objective-problem-statement}
The goal of this project is to provide a complete solution for RL algorithm evaluation that is extensible, scalable, and easy-to-use. It consists of four tightly integrated components (collectively called aiVLE 2.0):

\begin{enumerate}
    \item aiVLE\footnote{aiVLE is name for the grading system currently used in CS4246, more details about the old aiVLE (aiVLE 1.0) are covered in Section~\ref{ch:literature-review-related-work-ai-competition-platforms}} Gym: An OpenAI Gym \parencite{openai-gym} compatible RL environment framework with agent-environment separation and official support for multi-agent tasks.
    \item aiVLE Grader: An auto grading framework for aiVLE Gym tasks.
    \item aiVLE Worker: A security sandbox to execute arbitrary code submissions safely in a resource restricted environment. And a massively scalable worker client for evaluation.
    \item aiVLE Web: A web application for hosting RL competitions.
\end{enumerate}

\section{Motivation}
\label{s:project-objective-motivation}
Courses like CS4246, which teach AI, and are different from traditional programming/algorithm courses, need a system to automate the process of collecting and evaluating programming assignments on AI tasks. aiVLE 1.0 provides instant feedback on programming assignments that require varied computational power such as GPU or other specialized processing units. However, as mentioned in Section~\ref{ch:literature-review-related-work-ai-competition-platforms}, aiVLE 1.0 has many pain points. For example, it lacks extensibility (e.g., does not support multi-agent task), scalability (e.g., no concurrency safety for many workers), and documentation for more courses to make use of the platform. As for another similar platform called Botzone~\parencite{botzone}, although it is built for external users, limitations like no GPU support and no common interface make it infeasible for many tasks.

Secondly, an open source RL competition platform with good documentation and software engineering best practices has potential impact beyond education purposes. On the one hand, such a platform could also be useful for benchmarking AI algorithms – the consistency required for grading assignments is perfect for comparing research as well. On the other hand, examples like AlphaGo~\parencite{alphago} proves the effectiveness of combining supervised learning from past match data with unsupervised RL – match history collected on the platform could be useful for training models for corresponding tasks. 

Lastly, there is demand for such a platform. Both CS3243 and CS4246 have assignments on RL algorithms, and many non-RL tasks such as brute force or informed search, satisfiability problems, can be solved using similar execution loops. It is a safe bet that such a platform will benefit many AI courses (including CS2109S and CS3244) by using it alongside platforms like Kaggle to cover most AI algorithm evaluation scenarios.

\section{Roadmap}
\label{s:project-objective-roadmap}
There are three stages planned for this project. During the first semester, we delivered satisfactory results for the first two stages. During the winter break, we conducted \hyperref[ch:deployment-and-testing]{real-world deployment and testing} to evaluate the feasibility of advancing the project to the third stage. During the second semester, we focused on fixing problems found during the test, and delivering features as mentioned in the third stage.

\subsection{Stage 1: Framework}
Before developing the web platform for hosting the competitions, we first need frameworks for 1) creating environment and 2) evaluating agent’s performance – \hyperref[ch:aivle-gym]{aiVLE Gym} and \hyperref[ch:aivle-grader]{Grader} respectively. These frameworks are independent from the web platform, therefore could be used separately for other education or research purposes as well.
Besides being a platform-independent framework, a security sandbox, \hyperref[ch:aivle-worker]{aiVLE Worker}, is also necessary for hosting competitions. These three frameworks will be implemented in the mentioned order as the latter have dependency on the former.

\subsection{Stage 2: Platform}
With the frameworks ready, the second stage is to have \hyperref[ch:aivle-web]{a web platform} that achieves basic online judge functionality, which includes but is not limited to: 1) user registration and authentication, 2) creating/joining courses, 3) creating/modifying/submitting tasks, 4) showing the evaluation result. The target of this stage is to have a \emph{minimum viable product} that CS4246 can use to host single-agent assignments with GPU-accelerated evaluation.

\subsection{Stage 3: Advanced Solution}
Stage 1 and 2 provide a solid foundation for further extension and exploration, but primarily focus on delivering features that are necessary (thus \textbf{minimally} viable) for CS4246 teaching. Stage 3 aims to further extend the use case of our web platform. First, we will perform a real-world deployment and stress test of the system, which 1) validates the stability and performance gain of the new architecture, and 2) irons out unexpected behaviors and discrepancies in the deployment instructions. Second, we will improve the documentation of the source code and use cases, which is crucial for the maintainability and upgradability of this project. Lastly, we will add advanced features that are so-called ``quality-of-life updates''\footnote{There are also many other nice-to-have features that did not appear in this report, like more user-friendly frontend and course administration tools. To keep the report relatively concise, we only cover features that are deemed significant or technically challenging.} but are achievable within the time constraint (e.g., Sections \ref{ss:aivle-worker-resource-awareness} for resource awareness, \ref{ss:aivle-web-load-balancing} for load balancing).